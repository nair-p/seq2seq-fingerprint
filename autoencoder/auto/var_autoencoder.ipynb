{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "\n",
    "    def __init__(self, input_dim, learning_rate=1e-4, batch_size=2, n_z=16):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_z = n_z\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        self.build()\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        #self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "    # Build the netowrk and the loss functions\n",
    "    def build(self):\n",
    "        self.x = tf.placeholder(\n",
    "            name='x', dtype=tf.float32, shape=[None, input_dim])\n",
    "\n",
    "        # Encode\n",
    "        # x -> z_mean, z_sigma -> z\n",
    "        f1 = fc(self.x, 256, scope='enc_fc1', activation_fn=tf.nn.relu)\n",
    "        f2 = fc(f1, 128, scope='enc_fc2', activation_fn=tf.nn.relu)\n",
    "        f3 = fc(f2, 64, scope='enc_fc3', activation_fn=tf.nn.relu)\n",
    "        self.z_mu = fc(f3, self.n_z, scope='enc_fc4_mu', \n",
    "                       activation_fn=None)\n",
    "        self.z_log_sigma_sq = fc(f3, self.n_z, scope='enc_fc4_sigma', \n",
    "                                 activation_fn=None)\n",
    "        eps = tf.random_normal(\n",
    "            shape=tf.shape(self.z_log_sigma_sq),\n",
    "            mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.z_log_sigma_sq)) * eps\n",
    "\n",
    "        # Decode\n",
    "        # z -> x_hat\n",
    "        g1 = fc(self.z, 64, scope='dec_fc1', activation_fn=tf.nn.relu)\n",
    "        g2 = fc(g1, 128, scope='dec_fc2', activation_fn=tf.nn.relu)\n",
    "        g3 = fc(g2, 256, scope='dec_fc3', activation_fn=tf.nn.relu)\n",
    "        self.x_hat = fc(g3, input_dim, scope='dec_fc4', \n",
    "                        activation_fn=tf.sigmoid)\n",
    "\n",
    "        # Loss\n",
    "        # Reconstruction loss\n",
    "        # Mean-squared error loss\n",
    "        self.recon_loss = tf.reduce_mean(tf.squared_difference(self.x, self.x_hat))\n",
    "        \n",
    "        # Latent loss\n",
    "        # KL divergence: measure the difference between two distributions\n",
    "        # Here we measure the divergence between \n",
    "        # the latent distribution and N(0, 1)\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mu) - \n",
    "            tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "\n",
    "        self.total_loss = self.recon_loss + self.latent_loss\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.total_loss)\n",
    "        \n",
    "        self.losses = {\n",
    "            'recon_loss': self.recon_loss,\n",
    "            'latent_loss': self.latent_loss,\n",
    "            'total_loss': self.total_loss,\n",
    "        }        \n",
    "        return\n",
    "\n",
    "    # Execute the forward and the backward pass\n",
    "    def run_single_step(self, x):\n",
    "        _, losses = self.sess.run(\n",
    "            [self.train_op, self.losses],\n",
    "            feed_dict={self.x: x}\n",
    "        )\n",
    "        return losses\n",
    "\n",
    "    # x -> x_hat\n",
    "    def reconstructor(self, x):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.x: x})\n",
    "        return x_hat\n",
    "\n",
    "    # z -> x\n",
    "    def generator(self, z):\n",
    "        x_hat = self.sess.run(self.x_hat, feed_dict={self.z: z})\n",
    "        return x_hat\n",
    "    \n",
    "    # x -> z\n",
    "    def transformer(self, x):\n",
    "        z = self.sess.run(self.z, feed_dict={self.x: x})\n",
    "        return z\n",
    "    \n",
    "    # function to save model\n",
    "    def save_model(self, save_path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, save_path)\n",
    "        \n",
    "    # function to restore model\n",
    "    def restore_model(self, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model_object, tdata, num_samples, input_dim, learning_rate=1e-4, batch_size=2, num_epoch=10, n_z=16, log_step=2):\n",
    "    \n",
    "\n",
    "    model = model_object(input_dim, learning_rate=learning_rate, batch_size=batch_size, n_z=n_z)\n",
    "            \n",
    "        #saver = tf.train.Saver()\n",
    "    print(\"Training .... \\n\\n\")\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()\n",
    "            # Get a batch\n",
    "            # Execute the forward and backward pass \n",
    "            # Report computed losses\n",
    "        for iter in range( num_samples/batch_size):\n",
    "            epoch_input = tdata[iter * batch_size : (iter + 1) * batch_size]\n",
    "            losses = model.run_single_step(epoch_input)\n",
    "        end_time = time.time()\n",
    "        \n",
    "            #saver.save(sess=model.sess, save_path='./my_test_model.ckpt')\n",
    "        \n",
    "        if epoch % log_step == 0:\n",
    "            log_str = '[Epoch {}] '.format(epoch)\n",
    "            for k, v in losses.items():\n",
    "                log_str += '{}: {:.3f}  '.format(k, v)\n",
    "            log_str += '({:.3f} sec/epoch)'.format(end_time - start_time)\n",
    "            print(log_str)\n",
    "                    \n",
    "    print('\\nTraining done!\\n\\n')\n",
    "    \n",
    "    model.save_model('./model')\n",
    "    print(\"Saved model to disk\\n\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruction(model, tdata, num_samples, h=28, w=28, batch_size=2):\n",
    "    reconstructed = []\n",
    "    # Test the trained model: reconstruction\n",
    "    \n",
    "    print(\"Testing the model for reconstruction.\\nObtaining reconstructed seq2seq fingerprints \\n\\n\")\n",
    "    \n",
    "    print(\"Reconstructing %d samples ... \\n\\n\" % num_samples)\n",
    "    \n",
    "    reconst_counter = 0\n",
    "    \n",
    "    for iter in range(num_samples/batch_size):\n",
    "        \n",
    "        epoch_input = tdata[iter * batch_size : (iter + 1) * batch_size]\n",
    "    \n",
    "        x_reconstructed = model.reconstructor(epoch_input)\n",
    "        reconstructed.extend(x_reconstructed)\n",
    "        reconst_counter += batch_size\n",
    "        \n",
    "        if reconst_counter % 2 == 0:\n",
    "            print(\"Done reconstructing %d/%d lines\" % (reconst_counter, num_samples))\n",
    "        \n",
    "    print (\"\\nReconstruction done \\n\\n\")\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(filepath):\n",
    "    \n",
    "    # list to hold the string values \n",
    "    data = []\n",
    "    input_dim = 0\n",
    "    \n",
    "    # reading text file for training data\n",
    "    with open(train_filename, 'r') as File:\n",
    "        infoFile = File.readlines() #Reading all the lines from File\n",
    "        total_lines = len(infoFile)\n",
    "        print(\"Reading %d lines from training file\\n\\n\"%total_lines)\n",
    "\n",
    "        line_number = 0\n",
    "        \n",
    "        for line in infoFile: #Reading line-by-line\n",
    "            \n",
    "            line_number += 1\n",
    "            l = line[:-1].split()\n",
    "            tmp = []\n",
    "            for item in l:\n",
    "                tmp.append(float(item))\n",
    "            data.append(tmp)\n",
    "            \n",
    "            if line_number % 2 == 0:\n",
    "                print(\"Done with %d/%d lines\"%(line_number, total_lines))\n",
    "                \n",
    "            input_dim = max(input_dim, len(line.split()))\n",
    "\n",
    "    num_sample = len(data)\n",
    "    w = h = int(np.sqrt(input_dim))\n",
    "    \n",
    "    print(\"\\nData reading done\\n\\n\")\n",
    "    return data, num_sample, w, h, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 10 lines from training file\n",
      "\n",
      "\n",
      "Done with 2/10 lines\n",
      "Done with 4/10 lines\n",
      "Done with 6/10 lines\n",
      "Done with 8/10 lines\n",
      "Done with 10/10 lines\n",
      "\n",
      "Data reading done\n",
      "\n",
      "\n",
      "Training .... \n",
      "\n",
      "\n",
      "[Epoch 0] total_loss: 1.449  recon_loss: 0.566  latent_loss: 0.883  (0.027 sec/epoch)\n",
      "[Epoch 2] total_loss: 0.848  recon_loss: 0.560  latent_loss: 0.288  (0.014 sec/epoch)\n",
      "[Epoch 4] total_loss: 0.690  recon_loss: 0.550  latent_loss: 0.140  (0.014 sec/epoch)\n",
      "[Epoch 6] total_loss: 0.640  recon_loss: 0.553  latent_loss: 0.087  (0.012 sec/epoch)\n",
      "[Epoch 8] total_loss: 0.606  recon_loss: 0.542  latent_loss: 0.064  (0.014 sec/epoch)\n",
      "\n",
      "Training done!\n",
      "\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # path of text file containing SMILES strings for training and testing\n",
    "    train_filename = '../data/smiles_small.fp'\n",
    "    \n",
    "    # path of file to write reconstructed strings to for checking accuracy\n",
    "    reconst_filename = \"smiles_small_reconstructed.fp\"\n",
    "    \n",
    "    # Obtain the dataset for training \n",
    "    tdata, num_samples, w, h, input_dim = get_train_data(train_filename)\n",
    "    \n",
    "    # Train a model\n",
    "    model = trainer(VariationalAutoencoder, tdata, num_samples, input_dim)\n",
    "    \n",
    "    #saver = tf.train.Saver()\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    #model_json = model.to_json()\n",
    "    #with open(\"model_small.json\", \"w\") as json_file:\n",
    "    #    json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    #model.save_weights(\"model.h5\")\n",
    "    #saver.save(sess=model.sess, save_path='./my_test_model.ckpt')\n",
    "    #model.save_model_to_dir(\"./\", sess=model.sess)\n",
    "    \n",
    "    #with open(\"./model.ckpt\", \"wb\")\n",
    "    \n",
    "   \n",
    "    # load json and create model\n",
    "    #json_file = open('model.json', 'r')\n",
    "    #loaded_model_json = json_file.read()\n",
    "    #json_file.close()\n",
    "    #loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    #loaded_model.load_weights(\"model.h5\")\n",
    "    #saver.restore(sess=model.sess, './my_test_model.ckpt')\n",
    "    #print(\"Loaded model from disk\")\n",
    "    \n",
    "    #reconstructed = test_reconstruction(loaded_model, tdata, num_samples)\n",
    "    \n",
    "    # writing to a text file\n",
    "    #with open(reconst_filename, \"w\") as File:\n",
    "    #    for i,r in enumerate(reconstructed):\n",
    "    #        for item in r:\n",
    "    #            File.write(\"%f \" % item)\n",
    "    #        File.write(\"\\n\")\n",
    "\n",
    "    #File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Loaded model from disk\n",
      "Testing the model for reconstruction.\n",
      "Obtaining reconstructed seq2seq fingerprints \n",
      "\n",
      "\n",
      "Reconstructing 10 samples ... \n",
      "\n",
      "\n",
      "Done reconstructing 2/10 lines\n",
      "Done reconstructing 4/10 lines\n",
      "Done reconstructing 6/10 lines\n",
      "Done reconstructing 8/10 lines\n",
      "Done reconstructing 10/10 lines\n",
      "\n",
      "Reconstruction done \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Test the model for reconstruction\n",
    "loaded_model = VariationalAutoencoder(input_dim)\n",
    "loaded_model.restore_model('./model')\n",
    "print(\"Loaded model from disk\")\n",
    "    \n",
    "reconstructed = test_reconstruction(loaded_model, tdata, num_samples)\n",
    "    \n",
    "#writing to a text file\n",
    "with open(reconst_filename, \"w\") as File:\n",
    "    for i,r in enumerate(reconstructed):\n",
    "        for item in r:\n",
    "            File.write(\"%f \" % item)\n",
    "        File.write(\"\\n\")\n",
    "\n",
    "File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
